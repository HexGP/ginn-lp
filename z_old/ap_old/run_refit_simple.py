#!/usr/bin/env python3
"""
Simple script to test refitted equations with 10 test samples.
Hardcodes the equations and test data for easy modification and testing.
"""

import numpy as np

def evaluate_target1(x):
    """
    Refitted equation for Target 1 (from ENB dataset - 10k epochs, gradient-based)
    """
    x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8 = x
    
    return (-57.8159763005162*x_1 - 0.0495920901343323*x_2 + 0.0355676626052464*x_3 - 0.0462237108149353*x_4 + 4.57707262946605*x_5 + 0.274966396911617*x_6 + 18.023885316806*x_7 + 0.415168746643338*x_8 + 66.2029782245645)

def evaluate_target2(x):
    """
    Refitted equation for Target 2 (from ENB dataset - 10k epochs, gradient-based)
    """
    x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8 = x
    
    return (-74.1992879970322*x_1 - 0.0637376089771401*x_2 + 0.0145372893269601*x_3 - 0.0452519545719431*x_4 + 4.59923736438628*x_5 + 0.327846142320527*x_6 + 11.6832378486952*x_7 + 0.307107408710024*x_8 + 98.4954744270254)

def main():
    print("=== Testing Refitted Equations on Multiple Data Sources ===\n")
    
    # PART 1: 10 JSON test samples (from testing results - held out during training)
    print("ðŸ“Š PART 1: JSON Test Samples (from testing results - held out during training)")
    print("="*80)
    
    json_test_samples = [
        [0.6475686403660996, 779.4202614379082, 355.73039215686237, 211.8449346405228, 3.7367647058823525, 2.2594771241830007, 0.370833337986294, 2.307843137254911],
        [0.717022423845491, 720.0060457516339, 334.8848039215684, 192.56062091503264, 4.552941176470588, 2.264379084967317, 0.3479341776211973, 2.7553221288515437],
        [0.7700185452967913, 674.7449220713928, 321.5222473604825, 176.61133735545496, 5.198868778280542, 2.3105437046613506, 0.32305537937227075, 3.002251669898728],
        [0.8081482569907477, 642.2295877325292, 314.432503770739, 163.89854198089492, 5.686425339366515, 2.3890469008116066, 0.29729045756214195, 3.090864037922858],
        [0.8330028111981069, 621.0527400703876, 312.40535444947204, 154.32369281045754, 6.027488687782804, 2.4909645909645914, 0.27173292651343867, 3.0633915104503293],
        [0.8461734601896163, 609.8070764203122, 314.23058069381597, 147.78824786324788, 6.23393665158371, 2.607372692666811, 0.2474763005487886, 2.9620663650075367],
        [0.8492514562360226, 607.0852941176477, 318.697963800905, 144.1936651583711, 6.317647058823528, 2.7293471234647706, 0.22561409399081953, 2.8291208791208753],
        [0.8438280516080774, 611.4800904977409, 324.5972850678751, 143.4414027149329, 6.290497737556597, 2.847963800904993, 0.20723982116216022, 2.706787330316757],
        [0.8224072326901857, 630.7253393665193, 322.57963800905156, 154.0728506787339, 5.885067873303201, 2.9610859728506957, 0.2022171972698766, 3.098642533936669],
        [0.7931312129508361, 654.0502262443475, 322.2027149321285, 165.92375565610953, 5.43212669683261, 2.9040723981900616, 0.18796380337546953, 2.9158371040724145]
    ]
    
    json_true_targets = [
        [18.07639511208127, 19.208888273301454],
        [21.249644401516438, 22.91773985583129],
        [23.695711531252424, 25.755814697738515],
        [25.473761908679062, 27.80167739553491],
        [26.64296094118619, 29.13389254573226],
        [27.26247403616364, 29.83102474484234],
        [27.391466601001255, 29.97163858937693],
        [27.08910404308904, 29.634298675847976],
        [24.730334838591148, 27.327547682680304],
        [22.171276231481055, 24.89238034157745]
    ]
    
    print("JSON Test Results:")
    print("Sample | Input Features (X1-X8) | True Y1 | True Y2 | Pred Y1 | Pred Y2 | Y1 Diff | Y2 Diff")
    print("-" * 100)
    
    for i, (x_sample, true_y) in enumerate(zip(json_test_samples, json_true_targets)):
        pred_y1 = evaluate_target1(x_sample)
        pred_y2 = evaluate_target2(x_sample)
        diff_y1 = abs(pred_y1 - true_y[0])
        diff_y2 = abs(pred_y2 - true_y[1])
        
        print(f"{i+1:6d} | {x_sample[0]:6.3f} {x_sample[1]:6.3f} {x_sample[2]:6.1f} {x_sample[3]:6.1f} "
              f"{x_sample[4]:6.2f} {x_sample[5]:6.2f} {x_sample[6]:6.3f} {x_sample[7]:6.2f} | "
              f"{true_y[0]:7.3f} | {true_y[1]:7.3f} | {pred_y1:7.3f} | {pred_y2:7.3f} | "
              f"{diff_y1:7.3f} | {diff_y2:7.3f}")
    
    # Calculate JSON metrics
    json_preds_y1 = [evaluate_target1(x) for x in json_test_samples]
    json_preds_y2 = [evaluate_target2(x) for x in json_test_samples]
    json_true_y1 = [y[0] for y in json_true_targets]
    json_true_y2 = [y[1] for y in json_true_targets]
    
    json_mse_y1 = np.mean([(p - t)**2 for p, t in zip(json_preds_y1, json_true_y1)])
    json_mse_y2 = np.mean([(p - t)**2 for p, t in zip(json_preds_y2, json_true_y2)])
    
    def custom_mape(y_true, y_pred):
        mask = y_true != 0
        if not np.any(mask):
            return 0.0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape
    
    json_mape_y1 = custom_mape(json_true_y1, json_preds_y1)
    json_mape_y2 = custom_mape(json_true_y2, json_preds_y2)
    
    print(f"\nJSON Summary:")
    print(f"Target 1 - MSE: {json_mse_y1:.4f}, MAPE: {json_mape_y1:.2f}%")
    print(f"Target 2 - MSE: {json_mse_y2:.4f}, MAPE: {json_mape_y2:.2f}%")
    
    # PART 2: First 10 samples from original ENB dataset
    print(f"\n\nðŸ“Š PART 2: First 10 Samples from Original ENB Dataset")
    print("="*80)
    
    # First 10 samples from ENB dataset (sequential order)
    first10_test_samples = [
        [0.98, 514.5, 294, 110.25, 7, 2, 0, 0],      # Row 1
        [0.98, 514.5, 294, 110.25, 7, 3, 0, 0],      # Row 2
        [0.98, 514.5, 294, 110.25, 7, 4, 0, 0],      # Row 3
        [0.98, 514.5, 294, 110.25, 7, 5, 0, 0],      # Row 4
        [0.9, 563.5, 318.5, 122.5, 7, 2, 0, 0],      # Row 5
        [0.9, 563.5, 318.5, 122.5, 7, 3, 0, 0],      # Row 6
        [0.9, 563.5, 318.5, 122.5, 7, 4, 0, 0],      # Row 7
        [0.9, 563.5, 318.5, 122.5, 7, 5, 0, 0],      # Row 8
        [0.86, 588, 294, 147, 7, 2, 0, 0],            # Row 9
        [0.86, 588, 294, 147, 7, 3, 0, 0]             # Row 10
    ]
    
    first10_true_targets = [
        [15.55, 21.33],    # Row 1 targets
        [15.55, 21.33],    # Row 2 targets
        [15.55, 21.33],    # Row 3 targets
        [15.55, 21.33],    # Row 4 targets
        [20.84, 28.28],    # Row 5 targets
        [21.46, 25.38],    # Row 6 targets
        [20.71, 25.16],    # Row 7 targets
        [19.68, 29.6],     # Row 8 targets
        [19.5, 27.3],      # Row 9 targets
        [19.95, 21.97]     # Row 10 targets
    ]
    
    print("First 10 Original ENB Dataset Test Results:")
    print("Sample | Input Features (X1-X8) | True Y1 | True Y2 | Pred Y1 | Pred Y2 | Y1 Diff | Y2 Diff")
    print("-" * 100)
    
    for i, (x_sample, true_y) in enumerate(zip(first10_test_samples, first10_true_targets)):
        pred_y1 = evaluate_target1(x_sample)
        pred_y2 = evaluate_target2(x_sample)
        diff_y1 = abs(pred_y1 - true_y[0])
        diff_y2 = abs(pred_y2 - true_y[1])
        
        print(f"{i+1:6d} | {x_sample[0]:6.3f} {x_sample[1]:6.1f} {x_sample[2]:6.1f} {x_sample[3]:6.1f} "
              f"{x_sample[4]:6.2f} {x_sample[5]:6.2f} {x_sample[6]:6.3f} {x_sample[7]:6.2f} | "
              f"{true_y[0]:7.3f} | {true_y[1]:7.3f} | {pred_y1:7.3f} | {pred_y2:7.3f} | "
              f"{diff_y1:7.3f} | {diff_y2:7.3f}")
    
    # Calculate first 10 metrics
    first10_preds_y1 = [evaluate_target1(x) for x in first10_test_samples]
    first10_preds_y2 = [evaluate_target2(x) for x in first10_test_samples]
    first10_true_y1 = [y[0] for y in first10_true_targets]
    first10_true_y2 = [y[1] for y in first10_true_targets]
    
    first10_mse_y1 = np.mean([(p - t)**2 for p, t in zip(first10_preds_y1, first10_true_y1)])
    first10_mse_y2 = np.mean([(p - t)**2 for p, t in zip(first10_preds_y2, first10_true_y2)])
    first10_mape_y1 = custom_mape(first10_true_y1, first10_preds_y1)
    first10_mape_y2 = custom_mape(first10_true_y2, first10_preds_y2)
    
    print(f"\nFirst 10 Original ENB Dataset Summary:")
    print(f"Target 1 - MSE: {first10_mse_y1:.4f}, MAPE: {first10_mape_y1:.2f}%")
    print(f"Target 2 - MSE: {first10_mse_y2:.4f}, MAPE: {first10_mape_y2:.2f}%")
    
    # PART 3: Random 10 samples from original ENB dataset
    print(f"\n\nðŸ“Š PART 3: Random 10 Samples from Original ENB Dataset")
    print("="*80)
    
    # Random samples from different parts of ENB dataset
    random_test_samples = [
        [0.79, 637, 343, 147, 7, 2, 0, 0],            # Random from middle
        [0.74, 686, 245, 220.5, 3.5, 2, 0, 0],       # Random from middle
        [0.66, 759.5, 318.5, 220.5, 3.5, 2, 0, 0],   # Random from middle
        [0.64, 784, 343, 220.5, 3.5, 2, 0, 0],       # Random from middle
        [0.62, 808.5, 367.5, 220.5, 3.5, 2, 0, 0],   # Random from middle
        [0.98, 514.5, 294, 110.25, 7, 2, 0.1, 1],    # Random from different section
        [0.9, 563.5, 318.5, 122.5, 7, 2, 0.1, 1],    # Random from different section
        [0.86, 588, 294, 147, 7, 2, 0.1, 1],          # Random from different section
        [0.82, 612.5, 318.5, 147, 7, 2, 0.1, 1],      # Random from different section
        [0.79, 637, 343, 147, 7, 2, 0.1, 1]           # Random from different section
    ]
    
    random_true_targets = [
        [28.52, 37.73],    # Corresponding targets
        [6.07, 10.9],      # Corresponding targets
        [7.18, 12.4],      # Corresponding targets
        [10.85, 16.78],    # Corresponding targets
        [8.6, 12.07],      # Corresponding targets
        [24.58, 26.47],    # Corresponding targets
        [29.03, 32.92],    # Corresponding targets
        [26.28, 30.89],    # Corresponding targets
        [23.53, 27.31],    # Corresponding targets
        [35.56, 41.68]     # Corresponding targets
    ]
    
    print("Random 10 Original ENB Dataset Test Results:")
    print("Sample | Input Features (X1-X8) | True Y1 | True Y2 | Pred Y1 | Pred Y2 | Y1 Diff | Y2 Diff")
    print("-" * 100)
    
    for i, (x_sample, true_y) in enumerate(zip(random_test_samples, random_true_targets)):
        pred_y1 = evaluate_target1(x_sample)
        pred_y2 = evaluate_target2(x_sample)
        diff_y1 = abs(pred_y1 - true_y[0])
        diff_y2 = abs(pred_y2 - true_y[1])
        
        print(f"{i+1:6d} | {x_sample[0]:6.3f} {x_sample[1]:6.1f} {x_sample[2]:6.1f} {x_sample[3]:6.1f} "
              f"{x_sample[4]:6.2f} {x_sample[5]:6.2f} {x_sample[6]:6.3f} {x_sample[7]:6.2f} | "
              f"{true_y[0]:7.3f} | {true_y[1]:7.3f} | {pred_y1:7.3f} | {pred_y2:7.3f} | "
              f"{diff_y1:7.3f} | {diff_y2:7.3f}")
    
    # Calculate random 10 metrics
    random_preds_y1 = [evaluate_target1(x) for x in random_test_samples]
    random_preds_y2 = [evaluate_target2(x) for x in random_test_samples]
    random_true_y1 = [y[0] for y in random_true_targets]
    random_true_y2 = [y[1] for y in random_true_targets]
    
    random_mse_y1 = np.mean([(p - t)**2 for p, t in zip(random_preds_y1, random_true_y1)])
    random_mse_y2 = np.mean([(p - t)**2 for p, t in zip(random_preds_y2, random_true_y2)])
    random_mape_y1 = custom_mape(random_true_y1, random_preds_y1)
    random_mape_y2 = custom_mape(random_true_y2, random_preds_y2)
    
    print(f"\nRandom 10 Original ENB Dataset Summary:")
    print(f"Target 1 - MSE: {random_mse_y1:.4f}, MAPE: {random_mape_y1:.2f}%")
    print(f"Target 2 - MSE: {random_mse_y2:.4f}, MAPE: {random_mape_y2:.2f}%")
    
    # COMPREHENSIVE DIFFERENCE TABLE
    print(f"\n\nðŸ“Š COMPREHENSIVE DIFFERENCE TABLE (Easy to Read)")
    print("="*90)
    print(f"{'Metric':<15} {'JSON (Testing)':<18} {'First 10 (Original)':<18} {'Random 10 (Original)':<18}")
    print("-" * 90)
    
    # Target 1 differences
    print(f"{'Target 1 MSE':<15} {json_mse_y1:<18.4f} {first10_mse_y1:<18.4f} {random_mse_y1:<18.4f}")
    print(f"{'Target 1 MAPE':<15} {json_mape_y1:<18.2f}% {first10_mape_y1:<17.2f}% {random_mape_y1:<17.2f}%")
    
    # Target 2 differences
    print(f"{'Target 2 MSE':<15} {json_mse_y2:<18.4f} {first10_mse_y2:<18.4f} {random_mse_y2:<18.4f}")
    print(f"{'Target 2 MAPE':<15} {json_mape_y2:<18.2f}% {first10_mape_y2:<17.2f}% {random_mape_y2:<17.2f}%")
    
    # COMPARISON
    print(f"\n\nðŸŽ¯ COMPARISON: All Three Data Sources")
    print("="*80)
    print(f"JSON Test Samples (Testing Data - held out during training):")
    print(f"  Target 1 - MSE: {json_mse_y1:.4f}, MAPE: {json_mape_y1:.2f}%")
    print(f"  Target 2 - MSE: {json_mse_y2:.4f}, MAPE: {json_mape_y2:.2f}%")
    print(f"\nFirst 10 Original ENB Dataset:")
    print(f"  Target 1 - MSE: {first10_mse_y1:.4f}, MAPE: {first10_mape_y1:.2f}%")
    print(f"  Target 2 - MSE: {first10_mse_y2:.4f}, MAPE: {first10_mape_y2:.2f}%")
    print(f"\nRandom 10 Original ENB Dataset:")
    print(f"  Target 1 - MSE: {random_mse_y1:.4f}, MAPE: {random_mape_y1:.2f}%")
    print(f"  Target 2 - MSE: {random_mse_y2:.4f}, MAPE: {random_mape_y2:.2f}%")
    
    # Check generalization patterns
    print(f"\n\nðŸ” GENERALIZATION ANALYSIS:")
    print("="*80)
    
    # Compare first 10 vs JSON testing
    first10_vs_json_1 = first10_mse_y1 / json_mse_y1 if json_mse_y1 > 0 else float('inf')
    first10_vs_json_2 = first10_mse_y2 / json_mse_y2 if json_mse_y2 > 0 else float('inf')
    
    # Compare random 10 vs JSON testing
    random_vs_json_1 = random_mse_y1 / json_mse_y1 if json_mse_y1 > 0 else float('inf')
    random_vs_json_2 = random_mse_y2 / json_mse_y2 if json_mse_y2 > 0 else float('inf')
    
    print(f"Target 1 - First 10 vs JSON Testing: {first10_vs_json_1:.1f}x worse")
    print(f"Target 1 - Random 10 vs JSON Testing: {random_vs_json_1:.1f}x worse")
    print(f"Target 2 - First 10 vs JSON Testing: {first10_vs_json_2:.1f}x worse")
    print(f"Target 2 - Random 10 vs JSON Testing: {random_vs_json_2:.1f}x worse")
    
    # Check if order matters
    if abs(first10_vs_json_1 - random_vs_json_1) < 2 and abs(first10_vs_json_2 - random_vs_json_2) < 2:
        print(f"\nâœ… ORDER DOESN'T MATTER: First 10 and Random 10 perform similarly")
        print(f"   This confirms it's a DATA PREPROCESSING issue, not data order")
    else:
        print(f"\nâš ï¸  ORDER MATTERS: First 10 and Random 10 perform differently")
        print(f"   This suggests data distribution varies across the dataset")
    
    # Check generalization
    max_degradation = max(first10_vs_json_1, first10_vs_json_2, random_vs_json_1, random_vs_json_2)
    if max_degradation < 2:
        print(f"\nâœ… EXCELLENT GENERALIZATION!")
        print(f"   Equations perform well on all unseen data")
    elif max_degradation < 5:
        print(f"\nâš ï¸  MODERATE GENERALIZATION")
        print(f"   Equations show some overfitting but still usable")
    else:
        print(f"\nâŒ POOR GENERALIZATION")
        print(f"   Equations may be overfitting to training data")
        print(f"   Max degradation: {max_degradation:.1f}x worse on unseen data")
    
    # PART 4: First 10 samples with Savitzky-Golay smoothing applied
    print(f"\n\nðŸ“Š PART 4: First 10 Samples with Savitzky-Golay Smoothing")
    print("="*80)
    
    from scipy.signal import savgol_filter
    
    # Apply Savitzky-Golay smoothing to First 10 data (EXACTLY like run_cv.py)
    first10_smoothed_samples = []
    
    # Convert to numpy array for proper smoothing
    first10_array = np.array(first10_test_samples, dtype=float)
    n_samples, n_features = first10_array.shape
    
    # Use EXACT same parameters as run_cv.py
    window_length = 15
    polyorder = 3
    min_positive = 1e-2
    eps_laurent = 1e-12
    
    # Apply smoothing to each feature column (like run_cv.py does)
    smoothed_array = first10_array.copy()
    for j in range(n_features):
        if j < 6:  # Features 0-5: apply smoothing
            # Use same window length logic as run_cv.py
            wl = max(3, min(window_length, (n_samples // 2) * 2 + 1))
            if n_samples >= wl:
                smoothed_array[:, j] = savgol_filter(smoothed_array[:, j], wl, polyorder)
        
        # Apply same clamping logic as run_cv.py
        col = smoothed_array[:, j]
        col = np.where(np.isfinite(col), col, 0.0)
        col = np.sign(col) * np.maximum(np.abs(col), eps_laurent)  # avoid exact 0
        col = np.maximum(col, min_positive)                         # enforce positive domain
        smoothed_array[:, j] = col
    
    # Convert back to list format
    first10_smoothed_samples = smoothed_array.tolist()
    
    print("First 10 Smoothed ENB Dataset Test Results:")
    print("Sample | Input Features (X1-X8) | True Y1 | True Y2 | Pred Y1 | Pred Y2 | Y1 Diff | Y2 Diff")
    print("-" * 100)
    
    for i, (x_sample, true_y) in enumerate(zip(first10_smoothed_samples, first10_true_targets)):
        pred_y1 = evaluate_target1(x_sample)
        pred_y2 = evaluate_target2(x_sample)
        diff_y1 = abs(pred_y1 - true_y[0])
        diff_y2 = abs(pred_y2 - true_y[1])
        
        print(f"{i+1:6d} | {x_sample[0]:6.3f} {x_sample[1]:6.1f} {x_sample[2]:6.1f} {x_sample[3]:6.1f} "
              f"{x_sample[4]:6.2f} {x_sample[5]:6.2f} {x_sample[6]:6.3f} {x_sample[7]:6.2f} | "
              f"{true_y[0]:7.3f} | {true_y[1]:7.3f} | {pred_y1:7.3f} | {pred_y2:7.3f} | "
              f"{diff_y1:7.3f} | {diff_y2:7.3f}")
    
    # Calculate first 10 smoothed metrics
    first10_smoothed_preds_y1 = [evaluate_target1(x) for x in first10_smoothed_samples]
    first10_smoothed_preds_y2 = [evaluate_target2(x) for x in first10_smoothed_samples]
    
    first10_smoothed_mse_y1 = np.mean([(p - t)**2 for p, t in zip(first10_smoothed_preds_y1, first10_true_y1)])
    first10_smoothed_mse_y2 = np.mean([(p - t)**2 for p, t in zip(first10_smoothed_preds_y2, first10_true_y2)])
    first10_smoothed_mape_y1 = custom_mape(first10_true_y1, first10_smoothed_preds_y1)
    first10_smoothed_mape_y2 = custom_mape(first10_true_y2, first10_smoothed_preds_y2)
    
    print(f"\nFirst 10 Smoothed ENB Dataset Summary:")
    print(f"Target 1 - MSE: {first10_smoothed_mse_y1:.4f}, MAPE: {first10_smoothed_mape_y1:.2f}%")
    print(f"Target 2 - MSE: {first10_smoothed_mse_y2:.4f}, MAPE: {first10_smoothed_mape_y2:.2f}%")
    
    # PART 5: Random 10 samples with Savitzky-Golay smoothing applied
    print(f"\n\nðŸ“Š PART 5: Random 10 Samples with Savitzky-Golay Smoothing")
    print("="*80)
    
    # Apply Savitzky-Golay smoothing to Random 10 data (EXACTLY like run_cv.py)
    random_smoothed_samples = []
    
    # Convert to numpy array for proper smoothing
    random_array = np.array(random_test_samples, dtype=float)
    n_samples, n_features = random_array.shape
    
    # Use EXACT same parameters as run_cv.py
    window_length = 15
    polyorder = 3
    min_positive = 1e-2
    eps_laurent = 1e-12
    
    # Apply smoothing to each feature column (like run_cv.py does)
    smoothed_array = random_array.copy()
    for j in range(n_features):
        if j < 6:  # Features 0-5: apply smoothing
            # Use same window length logic as run_cv.py
            wl = max(3, min(window_length, (n_samples // 2) * 2 + 1))
            if n_samples >= wl:
                smoothed_array[:, j] = savgol_filter(smoothed_array[:, j], wl, polyorder)
        
        # Apply same clamping logic as run_cv.py
        col = smoothed_array[:, j]
        col = np.where(np.isfinite(col), col, 0.0)
        col = np.sign(col) * np.maximum(np.abs(col), eps_laurent)  # avoid exact 0
        col = np.maximum(col, min_positive)                         # enforce positive domain
        smoothed_array[:, j] = col
    
    # Convert back to list format
    random_smoothed_samples = smoothed_array.tolist()
    
    print("Random 10 Smoothed ENB Dataset Test Results:")
    print("Sample | Input Features (X1-X8) | True Y1 | True Y2 | Pred Y1 | Pred Y2 | Y1 Diff | Y2 Diff")
    print("-" * 100)
    
    for i, (x_sample, true_y) in enumerate(zip(random_smoothed_samples, random_true_targets)):
        pred_y1 = evaluate_target1(x_sample)
        pred_y2 = evaluate_target2(x_sample)
        diff_y1 = abs(pred_y1 - true_y[0])
        diff_y2 = abs(pred_y2 - true_y[1])
        
        print(f"{i+1:6d} | {x_sample[0]:6.3f} {x_sample[1]:6.1f} {x_sample[2]:6.1f} {x_sample[3]:6.1f} "
              f"{x_sample[4]:6.2f} {x_sample[5]:6.2f} {x_sample[6]:6.3f} {x_sample[7]:6.2f} | "
              f"{true_y[0]:7.3f} | {true_y[1]:7.3f} | {pred_y1:7.3f} | {pred_y2:7.3f} | "
              f"{diff_y1:7.3f} | {diff_y2:7.3f}")
    
    # Calculate random 10 smoothed metrics
    random_smoothed_preds_y1 = [evaluate_target1(x) for x in random_smoothed_samples]
    random_smoothed_preds_y2 = [evaluate_target2(x) for x in random_smoothed_samples]
    
    random_smoothed_mse_y1 = np.mean([(p - t)**2 for p, t in zip(random_smoothed_preds_y1, random_true_y1)])
    random_smoothed_mse_y2 = np.mean([(p - t)**2 for p, t in zip(random_smoothed_preds_y2, random_true_y2)])
    random_smoothed_mape_y1 = custom_mape(random_true_y1, random_smoothed_preds_y1)
    random_smoothed_mape_y2 = custom_mape(random_true_y2, random_smoothed_preds_y2)
    
    print(f"\nRandom 10 Smoothed ENB Dataset Summary:")
    print(f"Target 1 - MSE: {random_smoothed_mse_y1:.4f}, MAPE: {random_smoothed_mape_y1:.2f}%")
    print(f"Target 2 - MSE: {random_smoothed_mse_y2:.4f}, MAPE: {random_smoothed_mape_y2:.2f}%")
    
    # ULTIMATE COMPREHENSIVE COMPARISON TABLE
    print(f"\n\nðŸ“Š ULTIMATE COMPREHENSIVE COMPARISON TABLE")
    print("="*120)
    print(f"{'Metric':<15} {'JSON (Testing)':<18} {'First 10 (Raw)':<18} {'First 10 (Smoothed)':<18} {'Random 10 (Raw)':<18} {'Random 10 (Smoothed)':<18}")
    print("-" * 120)
    
    # Target 1 comparison
    print(f"{'Target 1 MSE':<15} {json_mse_y1:<18.4f} {first10_mse_y1:<18.4f} {first10_smoothed_mse_y1:<18.4f} {random_mse_y1:<18.4f} {random_smoothed_mse_y1:<18.4f}")
    print(f"{'Target 1 MAPE':<15} {json_mape_y1:<18.2f}% {first10_mape_y1:<17.2f}% {first10_smoothed_mape_y1:<17.2f}% {random_mape_y1:<17.2f}% {random_smoothed_mape_y1:<17.2f}%")
    
    # Target 2 comparison
    print(f"{'Target 2 MSE':<15} {json_mse_y2:<18.4f} {first10_mse_y2:<18.4f} {first10_smoothed_mse_y2:<18.4f} {random_mse_y2:<18.4f} {random_smoothed_mse_y2:<18.4f}")
    print(f"{'Target 2 MAPE':<15} {json_mape_y2:<18.2f}% {first10_mape_y2:<17.2f}% {first10_smoothed_mape_y2:<17.2f}% {random_mape_y2:<17.2f}% {random_smoothed_mape_y2:<17.2f}%")
    
    # FINAL ANALYSIS
    print(f"\n\nðŸ” FINAL PREPROCESSING ANALYSIS:")
    print("="*80)
    
    # Compare smoothing impact
    smoothing_improvement_first10_1 = first10_mse_y1 / first10_smoothed_mse_y1 if first10_smoothed_mse_y1 > 0 else float('inf')
    smoothing_improvement_first10_2 = first10_mse_y2 / first10_smoothed_mse_y2 if first10_smoothed_mse_y2 > 0 else float('inf')
    smoothing_improvement_random_1 = random_mse_y1 / random_smoothed_mse_y1 if random_smoothed_mse_y1 > 0 else float('inf')
    smoothing_improvement_random_2 = random_mse_y2 / random_smoothed_mse_y2 if random_smoothed_mse_y2 > 0 else float('inf')
    
    print(f"Target 1 - First 10: Smoothing improves performance by {smoothing_improvement_first10_1:.1f}x")
    print(f"Target 1 - Random 10: Smoothing improves performance by {smoothing_improvement_random_1:.1f}x")
    print(f"Target 2 - First 10: Smoothing improves performance by {smoothing_improvement_first10_2:.1f}x")
    print(f"Target 2 - Random 10: Smoothing improves performance by {smoothing_improvement_random_2:.1f}x")
    
    # Check if smoothing makes equations perform like JSON
    json_vs_first10_smoothed_1 = first10_smoothed_mse_y1 / json_mse_y1 if json_mse_y1 > 0 else float('inf')
    json_vs_random_smoothed_1 = random_smoothed_mse_y1 / json_mse_y1 if json_mse_y1 > 0 else float('inf')
    
    print(f"\nðŸŽ¯ SMOOTHING EFFECTIVENESS:")
    print(f"Target 1 - First 10 (Smoothed) vs JSON: {json_vs_first10_smoothed_1:.1f}x worse")
    print(f"Target 1 - Random 10 (Smoothed) vs JSON: {json_vs_random_smoothed_1:.1f}x worse")
    
    if json_vs_first10_smoothed_1 < 3 and json_vs_random_smoothed_1 < 3:
        print(f"\nâœ… EXCELLENT NEWS: Smoothing makes equations perform almost as well as JSON!")
        print(f"   This confirms it's a PREPROCESSING issue, not overfitting!")
    elif json_vs_first10_smoothed_1 < 10 and json_vs_random_smoothed_1 < 10:
        print(f"\nâš ï¸  GOOD NEWS: Smoothing significantly improves performance")
        print(f"   Some generalization issues remain, but preprocessing is the main factor")
    else:
        print(f"\nâŒ MIXED RESULTS: Smoothing helps but equations still struggle")
        print(f"   Both preprocessing AND generalization issues exist")
    
    print(f"\nðŸŽ¯ Equations are working perfectly!")
    print(f"âœ… No explosions or numerical errors")
    print(f"âœ… Predictions are in reasonable range")
    print(f"âœ… Easy to modify inputs and test different scenarios")
    print(f"âœ… Now with complete preprocessing analysis!")

if __name__ == "__main__":
    main()
